# üêæ ResNet-101: Clasificaci√≥n de Gato vs Perro con Interpretabilidad

Repositorio de una aplicaci√≥n completa (API + App) que implementa **ResNet-101** en PyTorch para clasificar im√°genes de **gatos** y **perros**, e incorpora un m√≥dulo de **interpretabilidad** para entender ‚Äúqu√© ve‚Äù la red en cada predicci√≥n. El foco es educativo y reproducible: c√≥digo modular, configuraci√≥n por YAML y utilidades de evaluaci√≥n/visualizaci√≥n.

---

## üéØ Objetivos

- **Clasificaci√≥n binaria** (cat vs dog) sobre im√°genes del usuario (archivo local o URL).
- **Interpretabilidad avanzada** para inspeccionar la decisi√≥n del modelo en cada imagen:
  - **Occlusion Sensitivity**: relevancia por parches.
  - **Integrated Gradients**: atribuciones acumuladas desde una referencia.
  - **Grad-CAM**: mapa de calor clase-espec√≠fico.
  - **Feature Maps (depth)**: activaciones intermedias por capa.
  - **Kernels (depth)**: filtros aprendidos (p. ej., Conv1).

---

## üñºÔ∏è Showcase (coloca aqu√≠ tus capturas/figuras)

**Vista general de la App (Home / Subida de imagen):**  
[ESPACIO RESERVADO PARA IMAGEN/GIF]

**Predicci√≥n b√°sica (Etiqueta + Scores):**  
[ESPACIO RESERVADO PARA IMAGEN/GIF]

**Predicci√≥n avanzada (Interpretabilidad por m√©todo):**  
- Grad-CAM  
  [ESPACIO RESERVADO PARA IMAGEN/GIF]
- Integrated Gradients (overlay)  
  [ESPACIO RESERVADO PARA IMAGEN/GIF]
- Occlusion Sensitivity  
  [ESPACIO RESERVADO PARA IMAGEN/GIF]
- Feature Maps (profundidad/capas)  
  [ESPACIO RESERVADO PARA IMAGEN/GIF]
- Kernels (filtros aprendidos)  
  [ESPACIO RESERVADO PARA IMAGEN/GIF]

---

## üß© Componentes principales

- **`resnet101/`**: implementaci√≥n del modelo (desde cero) y artefactos de experimentos.
  - `src/` ‚Äì arquitectura, bloques residuales, utilidades de guardado.
  - `model_trained/` ‚Äì pesos entrenados (gesti√≥n recomendada v√≠a Git LFS o enlace externo).
  - `experiments/` ‚Äì resultados y paneles generados.

- **`src/`**: API de inferencia (FastAPI) y utilidades.
  - `api/` ‚Äì routers (`/health`, `/predict`, `/predict/advanced`), errores, middleware, deps.
  - `inference/` ‚Äì pipeline de preprocesamiento ‚Üí forward ‚Üí postprocesado ‚Üí validaci√≥n.
  - `schemas/` ‚Äì contratos Pydantic v2 para requests/responses (incluye metadatos e im√°genes base64).
  - `utils/` ‚Äì configuraci√≥n, lectura de variables de entorno, paths, etc.
  - `tests/` ‚Äì pruebas de contrato y validaciones de entrada (pytest).

- **`app/`**: interfaz de usuario (Streamlit) para subir im√°genes/URLs y explorar explicaciones.

- **`data/`**: preparaci√≥n de datos y estad√≠sticas.
  - `processed/` ‚Äì carpeta de trabajo (no versionar datos brutos).
  - `pet_stats.json` ‚Äì medias/desviaciones para normalizaci√≥n reproducible.

- **`notebooks/`**: verificaci√≥n de flujo de datos y sanity checks del modelo.

- **`oxford_pets_binary_resnet101.yaml`**: configuraci√≥n del experimento (datos, modelo, optimizador, scheduler, device).

---

## üîç Flujo de inferencia

1. **Entrada**: archivo o URL ‚Üí validaci√≥n (MIME/shape).
2. **Preprocesamiento**: resize/center-crop ‚Üí tensor normalizado (estad√≠sticos cacheados).
3. **Modelo (ResNet-101)**: forward ‚Üí logits ‚Üí softmax.
4. **Salida b√°sica**: `label` + `scores` + `meta`.
5. **Salida avanzada**: a√±ade `artifacts` con paneles (PNG base64) para:
   - `gradcam_panel`
   - `integrated_gradients_overlay`
   - `occlusion_overlay`
   - `feature_maps_panel`
   - `kernels_panel`
   (e indicadores de error por panel si aplica.)

---

## üß† Interpretabilidad (resumen conceptual)

- **Grad-CAM**: resalta zonas espacialmente relevantes para la clase objetivo a partir de gradientes de capas profundas.
- **Integrated Gradients**: atribuye importancia a cada p√≠xel integrando el gradiente a lo largo de un camino desde una referencia (baseline) hasta la imagen.
- **Occlusion Sensitivity**: mide c√≥mo cambia la confianza al ‚Äúocultar‚Äù parches de la imagen (b√∫squeda local de relevancia).
- **Feature Maps**: muestran qu√© patrones intermedios activan las capas (texturas, bordes, partes).
- **Kernels**: visualiza filtros aprendidos en convoluciones (especialmente capas tempranas).

---

## üìä M√©tricas de referencia (validaci√≥n)

- P√©rdida (Val Loss)
- ROC-AUC
- Reporte de clasificaci√≥n (Precisi√≥n/Recall/F1 por clase)
- Matriz de confusi√≥n

[ESPACIO RESERVADO PARA TABLA/FIGURA DE M√âTRICAS]

---

## üîê Pesos y datos

- Los **pesos del modelo** pueden distribuirse por **Git LFS** o alojarse externamente (HuggingFace/Drive) y enlazarse en el README.
- Los **datos** deben respetar sus licencias; este proyecto usa **Oxford-IIIT Pet** con fines educativos.

---

## üó∫Ô∏è Rutas expuestas (API)

- `GET /health` ‚Üí Estado del servicio.
- `POST /predict` ‚Üí Predicci√≥n base: `label`, `scores`, `meta`.
- `POST /predict/advanced` ‚Üí Predicci√≥n + `artifacts` de interpretabilidad.

[ESPACIO RESERVADO PARA EJEMPLOS DE REQUEST/RESPONSE]

---

## ‚öôÔ∏è Reproducibilidad

- Configuraci√≥n centralizada en **YAML** (dataset, normalizaci√≥n, arquitectura, optimizador, scheduler).
- Estad√≠sticos de normalizaci√≥n cacheados en `data/pet_stats.json`.
- Semillas y dispositivos controlados (CPU/CUDA).

---

## üß≠ Roadmap

- Exportaci√≥n ONNX/TorchScript.
- Batch inference por carpeta/URL list.
- Panel de interpretabilidad adicional (SmoothGrad, Grad-CAM++).  
- Tests m√°s exhaustivos de contrato y regresi√≥n visual.

---

## üìÑ Licencia y cr√©ditos

- Uso **educativo y de investigaci√≥n**.
- Cita sugerida a **He et al. (2015)** por ResNet y a los trabajos originales de interpretabilidad correspondientes.
- Agradecimientos a la comunidad PyTorch y al dataset Oxford-IIIT Pet.

---

## üì¨ Contacto

- Issues y mejoras: usar el **Issue Tracker** del repositorio.
- Preguntas t√©cnicas: abrir un **Discussion** con el tag `help-wanted`.
